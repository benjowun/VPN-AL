%----------------------------------------------------------------
%
%  File    :  vpn_learning.tex
%
%  Author  :  Keith Andrews, IICM, TU Graz, Austria
% 
%  Created :  22 Feb 96
% 
%  Changed :  19 Feb 2004
% 
%----------------------------------------------------------------

\chapter{Model Learning}

\label{chap:Learning}

This chapter covers the learning and experiment setup. It showcases the many steps needed to learn the model of an \ac{ipsec} server, highlighting various design decisions. Additionally, implementation problems and our proposed solutions are discussed and presented. As until now we have been discussing learning algorithms from a theoretical standpoint, we will begin with a brief definition of automata learning terminology to use going forward that better suits the task of learning a reactive system. 

The goal of our setup is to learn a Mealy machine that models the \ac{sul}. We refer to it as the \textit{learned model}, or simply \textit{model}. To this end, we employ a learning algorithm, which requires an input alphabet $\Sigma$ of packets that are understood by the \ac{sul}. We refer to individual elements of the input alphabet as \textit{inputs}, whereas we refer to a chain of (multiple) inputs as an \textit{input sequence}. Each input of an input sequence will be executed on the \ac{sul} subsequently. We refer to one execution of our learning program as one learning attempt. A successful learning attempt is one that results in a correct behavioral model of the \ac{sul}. As we are working with Mealy machines, the term \textit{output query} is used instead of membership query.

% Setup
\todo{consider moving this entirely to the evaluation section}
\iffalse
\section{Environment Setup} \label{sec:learnenv}
% describe VMs, IPsec server software, configuration etc
We developed and tested our custom mapper using two VirtualBox 6.1 \acp{vm} running standard Ubuntu 22.04 LTS distributions. Both \acp{vm} were allotted \SI{4}{\giga\byte} of memory and one CPU core. All communication took place in an isolated virtual network to eliminate possible external influences. During learning attempts, all power saving options and similar potential causes of disruptions were disabled. Additionally, the \ac{ipsec} server was restarted before learning attempt to ensure identical starting conditions. We designated one \ac{vm} as the initiator and one as the responder to create a typical client-server setup. We chose the open source \ac{ipsec} implementation Strongswan\footnote{https://www.strongswan.org/} as our \ac{sul}. The Strongswan server was installed on the responder \ac{vm} and set to listen for incoming connections from the initiator \ac{vm}. We used the Strongswan version US.9.5/K5.15.0-25-generic, installed using the default Ubuntu package manager, apt. The Strongswan server was configured to use \acp{psk} for authentication and default recommended security settings. Additionally, it was configured to allow unencrypted notification messages, which we used to reset the connection during the learning process. The used Strongswan configuration files can be found in Appendix TODO \todo{appendix}. For learning, we used the Python library \textsc{AALpy}\footnote{https://github.com/DES-Lab/AALpy} version 1.2.9 in conjunction with the packet manipulation library Scapy\footnote{https://scapy.net/}, version 2.4.5. Significant effort was put into expanding the \ac{isakmp} Scapy module to support all packets required for \ac{ipsec} as the module lacked many features out-of-the-box. The provided Python script, \todo{this}\emph{IPSEC\_IKEv1\_SUL}\footnote{TODO: github or supplementary material link} demonstrates how we use the \textsc{AALpy} in conjunction with our custom mapper to communicate with and learn the model of an \ac{ipsec} server. Figure~\ref{fig:AALSetup} shows a typical learning attempt using two connected \acp{vm}. The right \ac{vm} shows the output of an underway learning attempt, while the left one shows the corresponding Strongswan server logs.
\fi

% Basic Model-Learning workflow
\section{Learning Setup} \label{subsec:learningenv}% half - quarter page
The automata of two separate \ac{ipsec} implementations were learned, namely strongSwan and libreswan. The \ac{ipsec} servers were installed and setup on the responder \ac{vm}, as detailed in Chapter~\ref{chap:Setup}. They were configured to listen for incoming connections from the initiator \ac{vm}. Our automata were learned using the Python automata learning library \textsc{AALpy}~\cite{software:aalpy} version 1.2.9, in conjunction with the packet manipulation library Scapy\footnote{\url{https://scapy.net/}}, version 2.4.5. \textsc{AALpy} supports deterministic, non-deterministic and stochastic automata, including support for various formalisms for each automata type. We chose deterministic Mealy machines to describe the \ac{ipsec} server. However, learning automata with \textsc{AALpy} follows the same basic process, regardless of the type of automata used. Significant effort was put into expanding the \ac{isakmp} Scapy module to support all packets required for \ac{ipsec} as the module lacked many features out-of-the-box. The provided Python script, \todo{this}\emph{IPSEC\_IKEv1\_SUL}\footnote{TODO: github or supplementary material link} demonstrates how \textsc{AALpy} can be used in conjunction with our custom mapper to communicate with, and learn the model of an \ac{ipsec} server.

\begin{figure}[h]
	\begin{tikzpicture} 
		\node (n1) [draw, minimum width=7em, minimum height=3.5em, align=left] at (0,1) {~Learning\\~Algorithm};
		\node (n2) [draw, minimum width=7em, minimum height=3.5em, align=left] at (4.4,1) {~Mapper};
		\node(n3) [draw, minimum width=7em, minimum height=3.5em,  align=left] at (8.8,1) {~Interface\\~(Initiator)};
		\node(n4) [draw, minimum width=7em, minimum height=3.5em,  align=left] at (13.2,1) {~SUL\\~(Responder)};
		\draw [->] ($(n1)+(3.5em,+0.5em)$) -> ($(n2)+(-3.5em,+0.5em)$) node[midway,above,align=center] {\scriptsize~Abstract\\\scriptsize~Input};
		\draw [->] ($(n2)+(-3.5em,-0.5em)$) -> ($(n1)+(3.5em,-0.5em)$) node[midway,below,align=center] {\scriptsize~Abstract\\\scriptsize~Output};
		
		\draw [->] ($(n2)+(3.5em,+0.5em)$) -> ($(n3)+(-3.5em,+0.5em)$) node[midway,above,align=center] {\scriptsize~Concrete\\\scriptsize~Input};
		\draw [->] ($(n3)+(-3.5em,-0.5em)$) -> ($(n2)+(3.5em,-0.5em)$) node[midway,below,align=center] {\scriptsize~Concrete\\\scriptsize~Output};
		
		\draw [<->] (n3) -> (n4) node[midway,above,align=center] {\scriptsize~UDP};
	\end{tikzpicture} 
	\caption{Automata Learning Setup}
	\label{fig:AALSetup}
\end{figure}

Figure \ref{fig:AALSetup} gives an overview of the learning process, adapted from Tappler et al.~\cite{tappler2017}. To begin, the learning algorithm sends abstract inputs chosen from the input alphabet to the mapper class, which converts it to concrete inputs. The concrete inputs are then sent to the \ac{sul}, by means of a communication interface. In our case, the mapper class comprises the major portion of our work in the establishment of the learning framework and converts the abstract words into actual \ac{ipsec} packets that can be sent to the \ac{sul} Strongswan server via UDP packets. This alphabet abstraction step simplifies the learning process, as learning the model for all possible inputs of an \ac{ipsec} server would be tedious at best. Additionally, the separation between abstract and concrete inputs/outputs allows for easy future modifications to the message implementations, including fuzzing support, as well as increasing the readability of our code. 

\begin{table}[h]
	\centering
	\begin{tabular}{|l|l|}
		\hline
		\rowcolor[HTML]{DAE8FC} 
		\hline
		\multicolumn{1}{|l|}{\cellcolor[HTML]{DAE8FC}\textbf{Protocol}} & \multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}\textbf{Input Alphabet}} \\ \hline
		ISAKMP SA                                                       & sa\_main                                                             \\
		KE                                                              & key\_ex\_main                                                        \\
		AUTH                                                            & authenticate                                                         \\
		IPSEC SA                                                        & sa\_quick                                                            \\
		ACK                                                             & ack\_quick                                                           \\ \hline
	\end{tabular}
	\caption{Mapping protocol to input alphabet names}
	\label{tab:map_prot_ia}
\end{table}

To begin learning an automaton with \textsc{AALpy}, one must first choose a suitable input alphabet encompassing the language known by the server, as well as the learning algorithm to be used. Our chosen input alphabet corresponds to the \ac{ike}v1 protocol messages shown in Figure \ref{fig:IKEv1}. The protocol messages map to abstract inputs of the input alphabet as shown in Table~\ref{tab:map_prot_ia}. We use both the $L^*$ and $KV$ algorithms for learning with a state prefix equivalence oracle that provides state-coverage by means of random walks started from each state. The equivalence oracle is used by the chosen learning algorithm to test for conformance between the current hypothesis and the \ac{sul}, giving either a counterexample on failure, or confirmation that the \ac{sul} has been learned successfully. This corresponds to an equivalence query. Additionally, several optional \textsc{AALpy} features were enabled, including caching and non-determinism checks to improve the learning process. An overview of the relevant learning algorithm initialization code can be seen in Listing \ref{lst:eqcode}. Line 3 shows the used input alphabet, Line 4 the used equivalence oracle and Line 5 the used learning algorithm. Both the equivalence oracle and learning algorithm take the input alphabet and an object representing an interface to the \ac{sul} as parameters, were the \ac{sul} interface is defined as shown below in \ref{lst:sulinterface} and can execute inputs on, as well as reset the the actual \ac{sul}. The equivalence oracle is also passed as a parameter to the learning algorithm with a few additional optional parameters specifying the type of automaton to learn and enabling non-determinism checking and caching.

\begin{lstlisting}[float=h, caption=Equivalence Query code, label=lst:eqcode, language=python]
	# Code example detailing AAL with AALpy
	
	input_al = ['sa_main', 'key_ex_main', 'authenticate', 'sa_quick', 
	'ack_quick']
	eq_oracle = StatePrefixEqOracle(input_al, sul, walks_per_state=10, walk_len=10)
	learned_ipsec = run_Lstar(input_al, sul, eq_oracle=eq_oracle, automaton_type='mealy', cache_and_non_det_check=True)
	
\end{lstlisting}

The \ac{sul} interface defines the \emph{step} and \emph{reset} methods, as can be seen in Listing \ref{lst:sulinterface}. \emph{step}, seen in Line 3, is used to execute input actions. \emph{reset}, show in lines 8-12, reverts the \ac{sul} to an initial state. An output query is a sequence of inputs executed on the \ac{sul}. Every input sequence is executed starting from an initial state, hence the need for a \emph{reset} method. Used in combination, \emph{step} and \emph{reset} allow asking output queries to the \ac{sul}. \emph{pre} is called before each output query and \emph{post} afterwards. The abstract input chosen from the input alphabet is passed on to the mapper class for further processing. Line 4 shows how a function, corresponding to the abstract input, is called in the mapper class and the return value (abstract output) is passed on to the learning algorithm. \\

\begin{lstlisting}[float=h, caption=SUL interface, label=lst:sulinterface, language=python]
	# code excerpt from IPSEC_IKEv1_SUL.py
	
	def step(self, input):
		func = getattr(self.ipsec, input)
		ret = func()
		return ret
	
	def pre(self):
		self.ipsec.reset()
	
	def post(self):
		self.ipsec.delete()
\end{lstlisting}

The mapper class implements methods for each communication step in a typical \ac{ipsec}-\ac{ike}v1 exchange, as described in Section \ref{chap:Preliminaries}, but referred to by their input alphabet name according to Table~\ref{tab:map_prot_ia}. This includes methods for \emph{sa\_main}, \emph{key\_ex\_main}, \emph{authenticate}, \emph{sa\_quick}, \emph{ack\_quick} packets. Additionally, the mapper class supports \emph{DELETE} messages. The \emph{DELETE} message is special in that it actually sends two packets which is required to delete all existing connections to the Strongswan server. It is critical for the correct functioning of \emph{reset} that this input is executed correctly, hence it requires the \ac{sul} state be checked after execution, which is not feasible during learning. For these two reasons, it was mostly left out of the learning process. Furthermore, the mapper class contains a variety of helper functions used to handle the decryption and encryption of packets as well as parse received informational messages. Informational messages are mainly used in \ac{ipsec} to return error codes when something goes wrong. To illustrate our mapper class, (simplified) excerpts from the \emph{sa\_main} method are shown in Listing \ref{lst:mapper1}. It shows how a Scapy packet is constructed out of many different individually configurable layers and fields, allowing for a high degree of flexibility and customizability. Line 5 shows how an \ac{isakmp} transform is created, encompassing various security parameters. This transform is packed into a \ac{isakmp} proposal packet first and then the resulting packet is packet into an \ac{isakmp} \ac{sa} packet in Line 6. The \ac{sa} packet is appended to a generic top level \ac{isakmp} packet in Line 8. In Line 9 the \ac{isakmp} packet is sent to the \ac{sul} and its response (if any) is received. The connection manager, initialized as \texttt{self.\_conn}, handles the actual sending and receiving logic and returns the server response already converted into a matching Scapy object. The Scapy response object then undergoes a retransmission check and is then parsed with relevant data being used to update local values, as indicated in lines 11-16.

\begin{lstlisting}[float=h, caption=Excerpt of sa\_main method code, label=lst:mapper1, language=python]
	# code excerpt from IPSEC_MAPPER.py
	
	def sa_main(self, ...):
	...
	tf = [('Encryption', 'AES-CBC'), ('KeyLength', 256), ('Hash', 'SHA'), ('GroupDesc', '1024MODPgr'), ('Authentication', 'PSK'), ('LifeDuration', 28800)]
	sa_body_init = ISAKMP_payload_SA(prop=ISAKMP_payload_Proposal(trans_nb=1, trans=ISAKMP_payload_Transform(num=1, transforms=tf)))
	
	policy_neg = ISAKMP(init_cookie=cookie_i, next_payload=1, exch_type=2)/sa_body_init
	resp = self._conn.send_recv_data(policy_neg)
	
	if (ret := self.get_retransmission(resp)): 
	# retransmission handling
	...
	
	# Response handling (checks response code, decrypts if necessary, updates relevant local values)
	...
\end{lstlisting}

The \ac{ipsec} packets generated by the mapper class are passed on to our communication class, which acts as an interface for the \ac{sul} and handles all incoming and outgoing UDP communication. Additionally, it parses responses from the \ac{sul} into valid Scapy packets and passes them on to the mapper class. The mapper class then parses the received Scapy packets and returns an abstract output code representing the received data to the learning algorithm. This code corresponds to the type of received message, or in the case of an error response (informational message), the error type. For fuzzing purposes, several common error types were grouped together into categories and the error category was used as the return value. Finally, the abstract error codes are returned to the learning algorithm which uses it update its internal data structures and improve its understanding of the \ac{sul} by updating the model. \\

\section{Design Decisions and Problems} \label{subsec:design}
% Design details advantages and disadvantages --> problems and solutions (2-3p)
We use the Python library Scapy to construct \ac{isakmp} packets as required by the \ac{ike}v1 protocol. More exactly, we use the \ac{isakmp} package that defines a generic top-level \ac{isakmp} package as well as several more specific payloads that it can contain. Parsing was made more difficult by the fact that Scapy does not support/implement all the packets required by \ac{ipsec}-\ac{ike}v1. To solve this problem, we implemented all missing packets in the Scapy \ac{isakmp} class and used this modified version. Specifically, we added support for \ac{isakmp} Informational packets, including resolving all commonly supported error codes, \ac{isakmp} Delete packets, NAT-D, additional SA attributes for \ac{isakmp} and ESP. Additionally, we improved the \ac{isakmp} Transform, Proposal and ID packets. In addition to all the \ac{isakmp} packets, our chosen automata learning algorithms require a \ac{sul} reset method to be able to return to an initial starting point after each query. We implement this using a combination of the \ac{isakmp} \emph{DELETE} request and general \ac{isakmp} informational error messages. While \emph{DELETE} alone works for established connections in phase two of \ac{ike}, we require informational error messages to trigger a reset in phase one, as delete does not work here sufficiently.

Each concrete mapping function in our mapper class can be run with a standard configuration, or with arbitrary values for the respective fields of the resulting packet. This allows us to learn different variations of the \ac{ipsec} server. For example, our mapper class allows us to very easily switch between learning the server model when presented with valid inputs, and the model of the server when introduced to invalid, malformed messages in combination with valid ones. Additionally, this design of the mapper functions will make fuzz testing specific protocol messages quite simple. The model of the server presented with malformed inputs will serve as the basis for future model based fuzz testing and can be seen in Chapter~\ref{chap:Evaluation}.

As inputs will be sent in many different, potentially unusual, combinations during learning, we require a robust framework that correctly handles the encryption and decryption of \ac{ipsec} messages. For key management, we simply store the current base-keys but keep track of \acp{iv} on a per \ac{mid} basis. Additionally, we keep track of the M-IDs of server responses to detect and handle retransmissions of old messages. Each request, we store the response for use in the next message and update affected key material as needed. Most notably, the \acp{iv} are updated almost every request and differ between M-IDs. Informational requests also handle their \acp{iv} separately from other message types. For each request that we send, if available, we try to parse the response, decrypting it if necessary and resetting or adjusting our internal variables as required to match the server. To keep track of all the different \acp{mid}, we use a Python dictionary to map \acp{mid} to relevant keying and \ac{iv} information. Usually, \acp{iv} are updated to the last encrypted block of the most recently sent or received message, though this behavior varies slightly between phases and for informational messages. Keeping track of \acp{iv} is required to continuously be able to parse encrypted server responses and extract meaningful information. Implementation of the mapper class, in particular encryption and decryption functionality, was hindered at times by unclear RFC-specifications, but this was overcome by manually comparing packet dumps and Strongswan logs to fix errors.\\

To ensure that we receive all responses, we add a timed wait for each server response. In the case of no response arriving during the wait, we directly return an empty \textit{None} response and need no further handling. Otherwise, we check the response \ac{mid} against our list of previous \acp{mid} to detect retransmissions. A retransmission is when the server returns a previously returned message in response to a new request. In this case, the retransmission \ac{mid} is the same for both responses. Our retransmission handling is covered in more detail in Section~\ref{sec:nondet}. If a retransmission is detected, depending on the configured retransmission-handling rule, it is either ignored or the corresponding previous response is returned. To save some time when not ignoring retransmissions, we keep a dictionary mapping \acp{mid} to their parsed response codes, allowing us to skip the parsing stage for retransmitted messages and return the saved previously parsed response directly. If no retransmission is detected, we check that the message type matches the expected one and if so, parse the message further to update local values and extract a response code. If the message type does not match, it is usually an informational message, indicating some sort of error. In this case, we decrypt the message using the corresponding parameters (as they are calculated and saved differently for informational messages), and return a code indicating the error being reported. Finally, we catch and log unimplemented message types, but this case should not occur during learning and is implemented mainly for later fuzzing.

Since testing and automata learning can be a very time-intensive process, we implemented several performance improvements to speed up the learning process. First, we reduced the timeouts down to a minimum amount needed to still get deterministic results. Next, we categorized the server informational responses according to their severity and impact and then grouped the most common ones together under the same abstract response code. This decreased the amount of states that had to be learned at the cost of some informational loss. However, since any deviations or non-deterministic behavior would have been caught by the learning framework, we are confident that no important information was lost. Finally we switched out the $L^*$ learning algorithm for $KV$, as $KV$ can be more performative for learning environments where output queries are expensive operations. As \ac{ike}v1 is a networks protocol with quite a bit of communication in each phase and we additionally have to implement small timeouts to wait for the server, each individual output query can take several seconds. With hundreds of output queries required to learn the \ac{ipsec} server, this results in a lot of time spent running the algorithm. Consequently, any decrease to the amount of output queries should, in theory, lead to an overall decrease in runtime. Since \textsc{AALpy} supports the $KV$ algorithm, switching between the two learning algorithms is as easy as setting a simple flag as shown in line three of Listing \ref{lst:newalg} below. The $KV$ algorithm required less output queries to learn the \ac{sul} and consequently significantly improved the speed at which models could be learned. The detailed comparison of runtime statistics between $L^*$ and $KV$ can be found in Chapter \ref{chap:Evaluation}.

\begin{lstlisting}[float=ht, caption=Switching Learning Algorithms, label=lst:newalg, language=python]
	# code excerpt from IPSEC_IKEv1_SUL.py
	
	if kv:
	learned_ipsec, info = run_KV(input_al, sul, eq_oracle, automaton_type='mealy', cex_processing='rs')
	else:
	learned_ipsec, info = run_Lstar(input_al, sul, eq_oracle=eq_oracle, automaton_type='mealy', cache_and_non_det_check=True)
\end{lstlisting}


% non-determinism fixes
\section{Combating Non-determinism} \label{sec:nondet}
Despite many precautions taken to create a disturbance-free learning environment, as detailed in Chapter~\ref{chap:Setup}, the \ac{ipsec} server still exhibited non-deterministic behavior, resulting in variance among the learned models. While the majority of learned models were identical, the outliers were significantly different, having differing amounts of states and transitions between them. To help decrease the remaining non-deterministic behavior, additional timeouts were added to all requests in order to give the server more time to correctly work through all incoming requests. This measure helped further decrease the amount of outlying automata learned, however it did not fully fix the issue. Examination of the outliers led to the discovery that all outlying behavior was concentrated around so-called retransmissions. Essentially, the \ac{ike} specification allows for previous messages to be retransmitted if deemed useful. A possible trigger could be the final message of an \ac{ike} exchange being skipped / lost. For example, if instead of an \emph{AUTH} message, the server receives a phase two \emph{IPSEC SA} message, the server would not know if it missed a message or if their was an error on the other parties side. According to the \ac{isakmp} specification in RFC 2408~\cite{rfc:isakmp}, the handling of this situation is unspecified, leaving the exact handling up to the implementations, however two possible methods are proposed. Firstly, if the \emph{IPSEC SA} message can be verified somehow to be correct, the server may ignore the missing message and continue as is. Secondly, the server could retransmit the message prior to the missing one to force the other party to respond in kind. Strongswan appears to implement these retransmissions and due to internal timeouts of connections, seem to trigger in a not-quite-deterministic fashion in phase two of \ac{ipsec} \ac{ike}v1 protocol.

While interesting for fingerprinting, as certain models were learned with a much higher frequency than the outliers and they contain a lot of information, a deterministic model was required to serve as a base case for model-based fuzzing. Therefore, checks were added in our mapper to allow for the ignoring of retransmissions. The retransmission-filtering can be easily enabled or disabled through a simple flag and works by checking the message ID of incoming server responses against a list of previous message IDs (excluding zero, as it is the default message ID for phase one). If a repeated message ID is found, it is flagged as a retransmission and depending on the current filtering rule, ignored. With this addition, non-deterministic behavior no longer occurred, allowing the learning of a very clean model, as shown in Chapter~\ref{chap:Evaluation}, Figure \ref{fig:reference}. As an additional method of dealing with non-determinism problems but still keeping retransmissions, non-determinism errors can be caught as they occur and the offending queries repeated several times. If upon the first rerun the non-determinism does not occur again, the existing value can be accepted as the correct one. If however, the non-determinism errors persist for a set amount of repetitions with the same constant server response, it is likely, that the original saved response was incorrect and it can be updated to the new response. With the non-determinism correcting added, the automata learning works without non-determinism errors and the learned models are consistent with one another.
