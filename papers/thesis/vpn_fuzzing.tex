%----------------------------------------------------------------
%
%  File    :  vpn_fuzzing.tex
%
%  Author  :  Benjamin Wunderling, TU Graz, Austria
% 
%  Created :  22 Feb 96
% 
%  Changed :  19 Feb 2004
% 
%----------------------------------------------------------------

\chapter{Fuzzing}

\label{chap:Fuzzing}

\section{Environment Setup} \label{sec:fuzzenv}
% describe VMs, IPsec server software, configuration etc
We ran all our fuzzing tests in the same virtual network setup we used for our automata learning, on the same Ubuntu 22.04 LTS distributions. We again designated one \ac{vm} as the initiator which would send the fuzzed messages and the other one as the responder to create a typical client-server setup. All settings on the used VMs remained the same as while learning to ensure that no discrepancies were introduced by different environment settings. The \ac{sut} was also the same Strongswan server used for learning. The major difference to learning is that for fuzzing, we no longer require \textsc{AALpy}. Our only real dependency, apart from our mapper class, is the Python fuzzing framework boofuzz, version 0.4.1, which we use for input generation. 

\subsection{Adapting the Model} \label{subsec:adapting_model}
While we had already successfully learned a (deterministic) model of the \ac{sul} when exposed to expected inputs, this proved to be not particularly useful for model-based fuzzing, as each fuzz case would be treated as new behavior. Instead, prior to fuzzing, we learn a new model, again with retransmission-filtering enabled, but this time also with an expanded input alphabet. In addition to the previous input alphabet letters, we add an erroneous version of each input letter that maps to an \ac{ike} packet with some sort of error or malformation, as shown in Listing \ref{lst:inputal_2}. This doubles the size of our input alphabet. The goal is to learn the behavior of the \ac{sul} when exposed to typical errors or malformed packets that could arise during fuzzing, to be able to filter away as much uninteresting information as possible and focus on more unusual behavior. An example of such a malformed packet could be an incorrect length field, a wrong hash value or simply an unsupported \ac{sa} option. Since we have designed our mapper class in such a way as to allow for easy manipulation of packets, this was an easy change to implement. Some additional server responses had to be parsed correctly, but all in all, not much had to be changed. The resulting model can be seen in Figure \ref{fig:withfilterwitherrors} and was used as our reference model while testing.  

\begin{lstlisting}[float=h, caption=The Updated Input Alphabet, label=lst:inputal_2, numbers=none, language=python]
	input_al = ['sa_main', 'key_ex_main', 'authenticate', 'sa_quick', 'ack_quick', 'sa_main_err', 'key_ex_main_err', 'authenticate_err', 'sa_quick_err', 'ack_quick_err']
\end{lstlisting}


\subsection{Fuzzing Setup} \label{subsec:fuzz_setup}
% tools used
As we had already designed our mapper class in such a way as to allow for easy fuzzing, the only thing missing was a source of values to use for fuzzing. For this purpose, we used the open source fuzzing library boofuzz\footnote{https://github.com/jtpereyda/boofuzz}, which is a successor of the popular Sulley\footnote{https://github.com/OpenRCE/sulley} fuzzer. Boofuzz is usually used by first defining a protocol in terms of blocks and primitives that define the contents of each message, and then using those definitions to generate large amounts mutated values for testing. However, as we already had a very flexible way of sending manipulated \ac{ipsec} packets, we decided to only use the data generation features of boofuzz, forgoing the protocol definitions. To get relevant fuzz values for each field, we mapped each fuzzable field to a specific boofuzz primitive data type and then used that to generate our data. We now have fuzz data generation, but are still lacking a way of choosing which runs to use for our tests, what exactly to fuzz and how to make use of our reference model.

% procedure --> 2 phases: p1 - prefiltering, p2 - boofuzz fuzz lists
The general procedure for model-based fuzzing is to fuzz the target system, while at the same time keeping track of the expected outputs on a reference model, to be able to identify new states and interesting behavior. To this end, we implemented a very simple parser for the .dot files of our learned model, as well as a converter between .dot files and state machines. Using this state machine, we can then simply see if a fuzzed input sent to the \ac{sut} results in the same next state and response as it does on our reference model, by simply following the edge correspondng to the input being fuzzed. If we notice a mismatch, we will have found a new state and hitherto unexplored behavior of  the \ac{sut}. While our fuzzer does follow this general pattern, due to the \ac{ipsec}-\ac{ike}v1 protocol being rather complicated, fuzzing every possible field for every possible combination and concatenation of inputs would be an immense task that goes far beyond the scope of this masters thesis. Instead, we implemented several techniques to limit the amount of fuzzing to be done in a way that aims to still maximize the chances of discovering new states and potential bugs. 

Firstly, instead of fuzzing every possible field of the protocol, we instead chose 5-10 key fields from each packet that looked to be the most impactful and representative for that type of message. We focused on length fields, \ac{sa} proposals and hashes / keys, but also added general fields, such as the responder / initiator cookies. All the chosen fields were added as parameters to their respective mapper class methods and default to their usual values. Packets can then have all or just some of their fields fuzzed, chosen randomly. Input data for chosen fields are then obtained from a boofuzz generator of a type matching the data of the field..

The next step was the run-generation phase in which we look to generate a set of runs consisting of input alphabet words, where one of the letters will be randomly chosen to be fuzzed. Our first idea was to go on random walks through the state machine and mirror the messages sent to the \ac{sut} as well, but the problem here at least for truely random walks was a lot of wasted queries in phase one and not enough state coverage in phase two. Therefore, since we had already generated a number of runs during model learning that guarantees state-coverage (at least for the learned automaton), we decided to repurpose those runs for fuzzing. The problem with this approach however, was that the resulting set of runs was rather large. So, in an effort to reduce the fuzzing space, we employed an additional filtering phase before the actual in-depth fuzzing. In this phase, we go through each run one by one and randomly designate one of its input alphabet letters as the fuzzing target. Then we test each fuzzable field of that packet in the context of the run with a greatly reduced set of fuzz values and compare the results to the expected outcomes using our state machine. If new behavior is found, the run and fuzz target passes the filtering. Runs in which no new behavior is discovered are discarded. This allows us to focus our resources on testing those configurations in which it is more likely to discover new behavior and therefore also bugs. And since many of the runs are very similar, this decreases the chance of discarding interesting runs, as chances are good that a run that contains at least a relevant subset of the discarded run will pass.

Following the automatic filtering, we go over the results and manually check and remove / reduce the number of identical or not relevant cases. For example, we noticed that every run in which cookies were fuzzed led to new states, due to new cookies indicating a completely new connection and since we learned the model with static initiator cookies, this will always lead to a new state. 
Finally, following the automatic and manual filtering, we arrived at a list of 175 runs, compared to roughly ten times the number before filtering. The filtered list of runs can be found in Appendix \todo{APPENDIX} and contains all the discovered runs that exhibited new behavior. Instead of our approach using existing runs and filtering, we also developed a more promising method of run generation using scored mutation that is described in Subsection \ref{subsec:mutations}.

\todo[inline]{Diagram for fuzzing process}

\subsection{Mutation-based testcase generation} \label{subsec:mutations}
\todo[inline]{Describe scored mutation approach}