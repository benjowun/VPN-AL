%----------------------------------------------------------------
%
%  File    :  vpn_fuzzing.tex
%
%  Author  :  Benjamin Wunderling, TU Graz, Austria
% 
%  Created :  22 Feb 96
% 
%  Changed :  19 Feb 2004
% 
%----------------------------------------------------------------

\chapter{Fuzzing}

\label{chap:Fuzzing}

\section{Environment Setup} \label{sec:fuzzenv}
% describe VMs, IPsec server software, configuration etc
We ran all our fuzzing tests in the same virtual network setup we used for our automata learning, on the same Ubuntu 22.04 LTS distributions. We again designated one \ac{vm} as the initiator which would send the fuzzed messages and the other one as the responder to create a typical client-server setup. All settings on the used VMs remained the same as while learning to ensure that no discrepancies were introduced by different environment settings. The \ac{sut} was also the same Strongswan server used for learning. The major difference to learning is that for fuzzing, we no longer require \textsc{AALpy}. Our only real dependency, apart from our mapper class, is the Python fuzzing framework boofuzz, version 0.4.1, which we use for input generation. 

\subsection{Adapting the Model} \label{subsec:adapting_model}
While we had already successfully learned a (deterministic) model of the \ac{sul} when exposed to expected inputs, this proved to be not particularly useful for model-based fuzzing, as each fuzz case would be treated as new behavior. Instead, prior to fuzzing, we learn a new model, again with retransmission-filtering enabled, but this time also with an expanded input alphabet. In addition to the previous input alphabet letters, we add an erroneous version of each input letter that maps to an \ac{ike} packet with some sort of error or malformation, as shown in Listing \ref{lst:inputal_2}. This doubles the size of our input alphabet. The goal is to learn the behavior of the \ac{sul} when exposed to typical errors or malformed packets that could arise during fuzzing, to be able to filter away as much uninteresting information as possible and focus on more unusual behavior. An example of such a malformed packet could be an incorrect length field, a wrong hash value or simply an unsupported \ac{sa} option. Since we have designed our mapper class in such a way as to allow for easy manipulation of packets, this was an easy change to implement. Some additional server responses had to be parsed correctly, but all in all, not much had to be changed. The resulting model can be seen in Figure \ref{fig:withfilterwitherrors} and was used as our reference model while testing.  

\begin{lstlisting}[float=h, caption=The Updated Input Alphabet, label=lst:inputal_2, numbers=none, language=python]
	input_al = ['sa_main', 'key_ex_main', 'authenticate', 'sa_quick', 'ack_quick', 'sa_main_err', 'key_ex_main_err', 'authenticate_err', 'sa_quick_err', 'ack_quick_err']
\end{lstlisting}


\subsection{Fuzzing Setup} \label{subsec:fuzz_setup}
% tools used
As we had already designed our mapper class in such a way as to allow for easy fuzzing, the only thing missing was a source of values to use for fuzzing. For this purpose, we used the open source fuzzing library boofuzz\footnote{https://github.com/jtpereyda/boofuzz}, which is a successor of the popular Sulley\footnote{https://github.com/OpenRCE/sulley} fuzzer. Boofuzz is usually used by first defining a protocol in terms of blocks and primitives that define the contents of each message, and then using those definitions to generate large amounts mutated values for testing. However, as we already had a very flexible way of sending manipulated \ac{ipsec} packets, we decided to only use the data generation features of boofuzz, forgoing the protocol definitions. To get relevant fuzz values for each field, we mapped each fuzzable field to a specific boofuzz primitive data type and then used that to generate our data. We now have fuzz data generation, but are still lacking a way of choosing which runs to use for our tests, what exactly to fuzz and how to make use of our reference model.

% procedure --> 2 phases: p1 - prefiltering, p2 - boofuzz fuzz lists
The general procedure for model-based fuzzing is to fuzz the target system, while at the same time keeping track of the expected outputs on a reference model, to be able to identify new states and interesting behavior. To this end, we implemented a very simple parser for the .dot files of our learned model, as well as a converter between .dot files and state machines. Using this state machine, we can then simply see if a fuzzed input sent to the \ac{sut} results in the same next state and response as it does on our reference model, by simply following the edge correspondng to the input being fuzzed. If we notice a mismatch, we will have found a new state and hitherto unexplored behavior of  the \ac{sut}. While our fuzzer does follow this general pattern, due to the \ac{ipsec}-\ac{ike}v1 protocol being rather complicated, fuzzing every possible field for every possible combination and concatenation of inputs would be an immense task that goes far beyond the scope of this masters thesis. Instead, we implemented several techniques to limit the amount of fuzzing to be done in a way that aims to still maximize the chances of discovering new states and potential bugs. 

Firstly, instead of fuzzing every possible field of the protocol, we instead chose 5-10 key fields from each packet that looked to be the most impactful and representative for that type of message. We focused on length fields, \ac{sa} proposals and hashes / keys, but also added general fields, such as the responder / initiator cookies. All the chosen fields were added as parameters to their respective mapper class methods and default to their usual values. Packets can then have all or just some of their fields fuzzed, chosen randomly. Input data for chosen fields are then obtained from a boofuzz generator of a type matching the data of the field..

The next step was the run-generation phase in which we look to generate a set of runs consisting of input alphabet words, where one of the letters will be randomly chosen to be fuzzed. Our first idea was to go on random walks through the state machine and mirror the messages sent to the \ac{sut} as well, but the problem here at least for truely random walks was a lot of wasted queries in phase one and not enough state coverage in phase two. Therefore, since we had already generated a number of runs during model learning that guarantees state-coverage (at least for the learned automaton), we decided to repurpose those runs for fuzzing. The problem with this approach however, was that the resulting set of runs was rather large. So, in an effort to reduce the fuzzing space, we employed an additional filtering phase before the actual in-depth fuzzing. In this phase, we go through each run one by one and randomly designate one of its input alphabet letters as the fuzzing target. Then we test each fuzzable field of that packet in the context of the run with a greatly reduced set of fuzz values and compare the results to the expected outcomes using our state machine. If new behavior is found, the run and fuzz target passes the filtering. Runs in which no new behavior is discovered are discarded. This allows us to focus our resources on testing those configurations in which it is more likely to discover new behavior and therefore also bugs. And since many of the runs are very similar, this decreases the chance of discarding interesting runs, as chances are good that a run that contains at least a relevant subset of the discarded run will pass.

Following the automatic filtering, we go over the results and manually check and remove / reduce the number of identical or not relevant cases. For example, we noticed that every run in which cookies were fuzzed led to new states, due to new cookies indicating a completely new connection and since we learned the model with static initiator cookies, this will always lead to a new state. 
Finally, following the automatic and manual filtering, we arrived at a list of 175 runs, compared to roughly ten times the number before filtering. The filtered list of runs can be found in Appendix \todo{APPENDIX} and contains all the discovered runs that exhibited new behavior. Instead of our approach using existing runs and filtering, we also developed a more promising method of run generation using scored mutation that is described in Subsection \ref{subsec:mutations}.

\todo[inline]{Diagram for fuzzing process}

\subsection{Mutation-based Testcase Generation} \label{subsec:mutations}
While the run generation method described above does work, in practice it is too slow without the added filtering stages. However, even with the added filtering stages, fuzzing the remaining 175 runs still took well over several days. As an alternative, we developed the following run generation method, which results in only a single run to be tested. The goal is to generate a run which has the highest possible chance of reaching the largest amount of interesting states. To this end, we designed a method for scored, mutation-based testcase generation. 

In its simplest form, mutation-based testcase generation refers to the generation of testcases, in our case runs, by repeatedly mutating a base case. In other words, we generate testcases by repeatedly applying small changes to a base testcase. These small changes, or mutations, could for example be swapping a bit, or changing a letter. In our fuzzer, we implemented two mutation operations. Adding a new letter of the input alphabet and swapping an existing letter in the run with its opposite version (so an erroneous version becomes a valid one, and vice versa). For our mutation base we can either use a random input alphabet letter, or pass a run to be used. We added this second option since \ac{ipsec} phase one packets have to be sent in a specific order to successfully authenticate. Remembering back to our attempt to use fully random runs for fuzzing, we know that phase two is explored far less than phase one. Therefore, we can start the mutations with phase one already completed if we so desire. 

At this point, we had implemented very basic mutation-based testcase generation, but the runs generated were often of low interest. As our goal is to generate as meaningful a run as possible, we decided to score the generated mutations and only keep a mutation, if its score is higher than its predecessor. Intuitively, the score given should serve as a representation of how easy it is to find new states while fuzzing a given run. To calculate the score of a mutated run, we test each fuzzable field of every letter in the run using the minimal list of fuzzing values used previously for our initial filtering phase and record the amount of new states discovered. To calculate the final score, we divide the total sum of new states by the length of the run. While the scoring does take some time, as we have to test at least five fields per letter, and we test every letter in the run compared to a randomly chosen one before, it is still much faster than the initial filtering phase provided the length of the mutated run does not come close to the number of runs need to learn the model. However, we do need to run this scoring step after every mutation, so for long mutations (>30 mutations) the total runtime can exceed that of the previous filtering step. The key difference is, that after the mutations are completed, the result will be a single run that is more interesting than any previous generated run, as opposed to the 175 runs after filtering. Finally, we weighted the mutation operations to make adding a new letter at the end of the word more likely than at a random index, as well as increasing the likelihood of the last letter being flipped over random letters. These changes try to decrease the chances of wasting time changing existing interesting configurations instead of adding to them, but still leaves the possibility given enough mutations. An example mutation in which prependeding the letter sa\_main to the existing run leads to a significant score increase can be seen in Listing \ref{lst:mutations}. The most interesting run found within 20 mutations is shown in line seven.

Comparing the two methods, the filtering one takes far longer, but does explore a much higher number of states. In contrast, the mutation-based approach is much faster, and focuses on a small sample of states, but generates that sample to be as interesting as possible in regards to the amount of new states that can be discovered through it.

\todo[inline]{These are not final results and still have the mentioned error, will update once I have the correct results}
\begin{lstlisting}[float=h, caption=Mutations, label=lst:mutations]
Mutation: 11, score: 82.0
['sa_main', 'key_ex_main_err', 'key_ex_main', 'sa_main', 'authenticate', 'sa_main', 'sa_quick', 'sa_main_err', 'authenticate']

Mutation: 12, score: 115.5
['sa_main', 'sa_main', 'key_ex_main_err', 'key_ex_main', 'sa_main', 'authenticate', 'sa_main', 'sa_quick', 'sa_main_err', 'authenticate']
...
Mutation: 16, score: 130.66666666666666
['sa_main', 'sa_main', 'key_ex_main_err', 'key_ex_main', 'sa_main', 'authenticate', 'sa_main', 'sa_quick', 'sa_main_err', 'ack_quick_err', 'sa_main_err', 'authenticate']
\end{lstlisting}
