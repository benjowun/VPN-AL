%----------------------------------------------------------------
%
%  File    :  vpn_evaluation.tex
%
%  Author  :  Keith Andrews, IICM, TU Graz, Austria
% 
%  Created :  22 Feb 96
% 
%  Changed :  19 Feb 2004
% 
%----------------------------------------------------------------

\chapter{Evaluation} \label{chap:Evaluation}

This chapter presents the results of our model learning and model-based fuzzing. Model learning results are presented in Section \ref{sec:learnresults}, beginning with the models learned using various input alphabets. Models from both examined \ac{ipsec} implementations are presented and discussed. The presentation of learned models is followed by a comparison of the two used learning algorithms, $L^*$ and $KV$, as well as the discussion of a library error found during learning. Finally, the fuzzing results are presented and discussed in Section \ref{sec:fuzzresults}, comparing the various methods of input sequence generation introduced in Chapter~\ref{chap:Fuzzing}.

\iffalse
\section{Environment Setup} \label{sec:env}
% describe VMs, IPsec server software, configuration etc
All model learning and testing took place in a virtual environment using two VirtualBox 6.1 \acp{vm} running standard Ubuntu 22.04 LTS distributions. Both \acp{vm} were allotted \SI{4}{\giga\byte} of memory and one CPU core. All inter-\ac{vm} communication took place in an isolated virtual network to eliminate possible external influences. During learning and fuzzing, all power saving options and similar potential causes of disruptions were disabled. Additionally, the \ac{ipsec} server was restarted before each learning attempt to ensure identical starting conditions. One \ac{vm} was designated as the initiator and one as the responder to create a typical client-server setup. We chose the open source \ac{ipsec} implementation Strongswan~\cite{software:strongswan} as our \ac{sul}. The Strongswan server was installed on the responder \ac{vm} and set to listen for incoming connections from the initiator \ac{vm}. We used the Strongswan version US.9.5/K5.15.0-25-generic, installed using the default Ubuntu package manager, apt. The Strongswan server was configured to use \acp{psk} for authentication and default recommended security settings. Additionally, it was configured to allow unencrypted notification messages, which were used to reset the connection during the learning process. Our Strongswan configuration files can be found in Appendix TODO \todo{appendix}. The Python library \textsc{AALpy}~\cite{software:aalpy} version 1.2.9 was used in conjunction with the packet manipulation library Scapy\footnote{\url{https://scapy.net/}}, version 2.4.5, in order to learn a model of the \ac{sut}. Significant effort was put into expanding the \ac{isakmp} Scapy module to support all packets required for \ac{ipsec} as the module lacked many features out-of-the-box. The provided Python script, \todo{this}\emph{IPSEC\_IKEv1\_SUL}\footnote{TODO: github or supplementary material link} demonstrates how \textsc{AALpy} can be used in conjunction with our custom mapper to communicate with and learn the model of an \ac{ipsec} server. Figure~\ref{fig:AALSetup} shows a typical learning attempt using two connected \acp{vm}. The right \ac{vm} shows the output of an underway learning attempt, while the left one shows the corresponding Strongswan server logs.
\fi

\section{Learning Results} \label{sec:learnresults}
% section where we show and analyze reference and no filter models including model and statistics
Over the course of our work, we learned a variety of different models due to different retransmission-handling settings and choice of inputs alphabets. The following sections showcase the four most relevant ones, all learned from a Linux Strongswan U5.9.5 server, using both the $KV$ and $L^*$ learning algorithms. Error codes have been simplified for better readability. As our \ac{sul} had some issues with non-determinism while retransmissions were enabled, one major differentiating factor in our models is whether retransmission-filtering was enabled for the learning process. This had a significant impact on the resulting learned model, with the version without filtering boasting more than twice the number of states than the one with. Additionally, even when using the methods to combat non-determinism described in Chapter \ref{chap:Learning}, Section \ref{sec:nondet} the resulting models still occasionally differed when not filtering out retransmissions. Therefore, the non-filtered models were not used for fuzzing, as a completely deterministic model was desired to serve as our baseline when fuzzing the \ac{sut}.

\subsection{Learning Metrics} \label{subsec:metrics}
The comparison of learned models and model learning algorithm performance in subsections \ref{subsec:models} and \ref{subsec:comp_kv_lstar} is based largely on the the following metrics, saved during the model learning process.

\subsubsection*{Steps}
Steps refers to the number of algorithm steps required by the learning algorithm or the equivalence checking.

\subsubsection*{Queries}
Queries refers to the amount of queries sent during state exploration (membership queries) or during conformance checking (equivalence queries). \textsc{AALpy} supports speeding up model learning by using caching to reduce the number of required membership queries. 

\subsubsection*{Runtime}
Runtime refers to the time it took to learn the model. It is further split into state exploration and conformance checking runtimes. Runtime directly correlates to the number of steps and queries. When given in seconds, the runtime is rounded to the nearest second. Note that \textsc{AALpy} refers to output queries as membership queries.


\subsection{Learned Models} \label{subsec:models}
Figures \ref{fig:ret_case1} and \ref{fig:ret_case2} show the two most commonly learned models when not filtering retransmissions. Roughly 80\% of all models learned without retransmission filtering enabled resulted in one of these two models, which we will refer to as the common models. The other 20\% of models were a non-uniform assortment of outliers. Figure \ref{fig:reference} shows the clean base model learned from the \ac{sul} with retransmission filtering enabled. The reference model used for fuzzing is shown in Figure \ref{fig:withfilterwitherrors}, also learned with retransmission-filtering enabled, as well as an expanded input alphabet. DOT files of all models are provided in Appendix \todo[inline]{APPENDIX/additional resources}. The runtimes of both learning algorithms for all four models are summarized in tables \ref{tab:runtime_summary_kv} and \ref{tab:runtime_summary_lstar}. All values are averages over multiple learning attempts. Tables use the following abbreviations:

\begin{enumerate}
	\item States: The number of states in the learned model
	\item TT (s): Total time needed to learn the model (in seconds)
	\item TL (s): Time spent on state exploration (in seconds)
	\item TC (s): Time spent on conformance checking (in seconds)
	\item MQ: Number of membership queries sent during the model learning
	\item EQ: Number of equivalence queries sent during model learning
\end{enumerate}
 

\subsubsection*{First Common Model}

The first common model, presented in Figure \ref{fig:ret_case1}, took approximately 52 minutes (3092 seconds) to learn with the $KV$ algorithm, spread over seven learning rounds. The model consists of 10 states. Of the 52 minutes total, roughly half were used for state exploration / membership queries and the other half for conformance checking, with conformance checking taking slightly longer (1501 vs 1591 seconds). 171 membership queries were performed by the learning algorithm in 2047 steps, whereas 100 equivalence queries were performed for conformance checking in 1826 steps.

In contrast, when learned with the $L^*$ algorithm, model learning took almost 85 minutes (5094 seconds) over five learning rounds. Here, the split between state exploration and conformance checking was more distinct, with state exploration taking up approximately 68\% of the total runtime and conformance checking only requiring the remaining 32\% (3489 vs 1605 seconds). 462 membership queries were required compared to the 171 of the $KV$ algorithm. Notably, the time needed for conformance checking remained largely the same between the two algorithms, however the difference in state exploration / membership queries is quite large. This behavior is discussed in more detail in Subsection \ref{subsec:comp_kv_lstar}, which includes a statistical comparison of the two algorithms.

Moving on to an examination of the first common model itself, we can clearly see a separation between the two phases. Phase one completes in state \emph{s3}, and phase two begins right thereafter. While phase one looks very clean and is in fact identical to the model learned with retransmission-filtering enabled, phase two has many complicated transitions caused by retransmissions. For example, all three transitions from state \emph{s5} to \emph{s7} via \emph{authenticate}, \emph{sa\_main} and \emph{key\_ex\_main}, highlighted in yellow, return a valid \emph{IPSEC SA} response. This should be impossible, as phase one messages are ignored while in phase two. However, due to specific timings of retransmissions, our communication interface can occasionally happen to be listening for a server response of a regular phase two communication, when the \ac{sul} sends a retransmission for previous \emph{sa\_quick} message. This causes our framework to treat the received retransmission as the response for the phase two message, when in fact, it is not. We can see multiple incoming and outgoing transitions of state \emph{s4}, highlighted in red, that further exhibit this same same behavior.
Another noticeable property of the learned automata, is that past state \emph{s2}, no paths lead back to the initial state. This is due to the fact that our input alphabet for this learned model does not include the delete command. Adding delete adds transitions from every state back to the initial one, but also dramatically increases the runtime and non-deterministic behavior of the \ac{sul}, as even more retransmissions are triggered. While not part of our input alphabet, it could be included in future work.

\begin{figure}[ht]
	\vspace*{\fill}
	\noindent
	\hspace*{-2.1\oddsidemargin}%
	\makebox[0pt][l]{\includegraphics[width=\linewidth, angle=270]{images/models/retransmissions/retrans_case1_lstar}}
	\caption{First commonly learned model with retransmissions.}
	\label{fig:ret_case1}
	\vspace*{\fill}
\end{figure}
\newpage

\subsubsection*{Second Common Model}

The second common model, seen in Figure \ref{fig:ret_case2}, took approximately 75 minutes (4507 seconds) to learn using the $KV$ algorithm. The model took nine rounds to learn, and consists of 12 states. Of those 75 minutes, roughly 53\% were used for state exploration / membership queries and the other 47\% (2382 vs 2126 seconds). 215 membership queries were performed by the learning algorithm in 2219 steps, whereas 120 equivalence queries were performed for conformance checking in 1964 steps. 

In contrast, when learned with the $L^*$ algorithm, model learning took significantly longer, running for 125 minutes (7520 seconds) over five learning rounds. Here, the split between state exploration and conformance checking was again very distinct, with state exploration taking up approximately 71\% of the total runtime and conformance checking only requiring the remaining 29\% (5393 vs 2126 seconds). Again, the time needed for conformance checking remained largely the same between the two algorithms, however the difference in state exploration / membership queries is even larger, with $L^*$ taking 522 membership queries.

Examining the model, we can again see a clear separation between the two phases. Phase one for this model is identical to the previous one, as no retransmission occur there. Same as in Figure \ref{fig:ret_case1}, no paths past state \emph{s2} lead back to the initial state. Phase two shows retransmission-induced strange behavior in the transitions \emph{s5} to \emph{s7}, as well as \emph{s11} to \emph{s9}. The strange behavior is again linked to retransmissions, causing phase one inputs, such as \emph{sa\_main}, to result in the valid phase two outputs, such as \emph{IPSEC SA}. The states \emph{s7} and \emph{s11} are separated by two states that do not exhibit any strange behavior, apart from having identical in and outputs. The main difference to the first common model is, that strange behavior occurs in two pairs of states, highlighted in yellow, and that these pairs are separated by two states that do not appear to receive any retransmissions, highlighted in red. This is likely caused by the \ac{sut} sending repeated retransmissions, in the same frequency, allowing for two states in between. 

\begin{figure}[ht]
	\vspace*{\fill}
	\noindent
	\hspace*{-2.1\oddsidemargin}%
	\makebox[0pt][l]{\includegraphics[width=\linewidth, angle=270]{images/models/retransmissions/retrans_case2_lstar}}
	\caption{Second commonly learned model with retransmissions.}
	\label{fig:ret_case2}
	\vspace*{\fill}
\end{figure}
\newpage

\subsubsection*{Clean Base Model}

In comparison, when learning the same server using retransmission-filtering, all non-deterministic behavior vanishes and we get the model shown in Figure \ref{fig:reference} every learning attempt. The model has only 6 states and therefore was learned much more quickly than the previous ones, with learning requiring only approximately 21 minutes (1266 seconds) using the $KV$ algorithm. Learning happened over four rounds, where the time was distributed between state exploration and conformance checking in a 40-60 split (519 vs 747 seconds). This was the only configuration where the conformance checking took longer than state exploration, as highlighted in Table \ref{tab:runtime_summary_kv}. It does however still have the lowest altogether runtime. In comparison, when learned with the $L^*$ algorithm, learning took roughly 36 minutes (2157 seconds), spread over two learning rounds. Of that time, state exploration required roughly 55\% compared to the 45\% needed for conformance checking (1188 vs 969 seconds). Compared to $KV$, state exploration / membership queries took more than twice the amount of time to complete.

Looking at the resulting model more closely, the first four states are again identical to the previous model. This is due to the fact that the retransmissions only triggered for phase two messages and since they are our only source of non-determinism, there are no differences here. However, the phase two states look wildly different, showing a streamlined behavior that fits our reference \ac{ike} exchange (see Figure \ref{fig:IKEv1}) almost perfectly. The only small difference lies in the additional state \emph{s5} which loops back to state \emph{s4} with an \emph{IPSEC SA} or \emph{ACK} message. This behavior shows how multiple \ac{ipsec} \acp{sa}, each created from a single IKE SA channel, can be used interchangeably for different traffic flows but not simultaneously. As soon as a new \ac{ipsec} \ac{sa} has been established, another \emph{ACK} message can be sent, to finalize the creation of the new \ac{ipsec} \ac{sa}. In other words, the extra state is there to show that a single \ac{ipsec} \ac{sa} cannot be acknowledged twice, and instead a new \ac{sa} must be created first.

\begin{figure}[ht]
	\vspace*{\fill}
	\noindent
	\hspace*{-2.1\oddsidemargin}%
	\makebox[0pt][l]{\includegraphics[width=\linewidth]{images/models/Reference}}
	\caption{Clean model learned using retransmission filtering}
	\label{fig:reference}
	\vspace*{\fill}
\end{figure}

\begin{table}[h]
	\centering
	\begin{tabular}{|l|l|l|l|l|l|l|}
		\hline
		\rowcolor[HTML]{C0C0C0} 
		Model     & States & TT (s)   & TL (s)   & TC (s)   & MQ  & EQ  \\ \hline
		Common A  & 10     & 5094 & 3489 & 1605 & 462 & 100 \\ \hline
		Common B  & 12     & 7520 & 5393 & 2126 & 522 & 120 \\ \hline
		Base      & 6      & 1652 & 899  & 753  & 177 & 60  \\ \hline
		Reference & 6      & 3078 & \textbf{2500} & 578  & 600 & 60  \\ \hline
	\end{tabular}
	\caption{$L^*$ Runtimes of all the learned models}
	\label{tab:runtime_summary_lstar}
\end{table}
\newpage

\subsubsection*{Fuzzing Reference Model}
% Error model
Figure \ref{fig:withfilterwitherrors} shows the reference model used for fuzzing, learned with retransmission-filtering enabled. Additionally, the input alphabet was expanded to include an additional erroneous version of each letter that maps to an erroneous input. Thanks to the retransmission-filtering, this model was, like the base model, 100\% deterministic. Using the $KV$ algorithm, it took roughly 24 minutes (1447 seconds) to learn. The model took between five and four learning rounds to learn and consists of six states. Roughly 60\% of the total learning time was spent on state exploration/membership queries and the remaining 40\% on conformance checking (879 vs 568 seconds). Alternatively, when learned using the $L^*$ algorithm, the model took a total of 51 minutes (3078 seconds) to learn over a single learning round. The 51 minutes were split between state exploration and conformance checking in a 81-19 split (2500 vs 578 seconds), with state exploration requiring 600 membership queries. Interesting to note is the large difference in conformance checking runtime between the two algorithms. For learning the reference model using $L^*$, state exploration takes up more than 80\% of the total runtime, which is the highest percentage for all the learned models, as can been seen highlighted in the runtime summary in Table \ref{tab:runtime_summary_lstar}.

The model looks largely identical to the previous model, apart from some additional self-transitions and one additional error transition from state \emph{s4} to \emph{s5}. Here, state \emph{s4} corresponds to the previous \emph{s6}. The error transition simply means that a valid \ac{ipsec} \ac{sa} must be created before it can be acknowledged.

\TODO{move model to appendix / mention DOT file in appendix}

\begin{figure}[ht]
	\vspace*{\fill}
	\noindent
	\hspace*{-4.5\oddsidemargin}%
	\makebox[0pt][l]{\includegraphics[width=\paperwidth]{images/models/WithFilterWithErrors_kv}}
	\caption{Model with malformed messages}
	\label{fig:withfilterwitherrors}
	\vspace*{\fill}
\end{figure}

\begin{table}[h]
	\centering
	\begin{tabular}{|l|l|l|l|l|l|l|}
		\hline
		\rowcolor[HTML]{C0C0C0} 
		Model     & States & TT (s)  & TL (s)  & TC (s)  & MQ  & EQ  \\ \hline
		Common A  & 10     & 3092 & 1501 & 1591 & 171 & 100 \\ \hline
		Common B  & 12     & 4507 & 2382 & 2126 & 215 & 120 \\ \hline
		Base      & 6      & 1214 & \textbf{480}  & 734  & 78  & 60  \\ \hline
		Reference & 6      & 1447 & 879  & 568  & 174 & 60  \\ \hline
	\end{tabular}
	\caption{KV Runtimes of all the learned models}
	\label{tab:runtime_summary_kv}
\end{table}
\newpage



\subsection{Comparing $KV$ and $L^*$} \label{subsec:comp_kv_lstar}
% section copmaring KV and Lstar
Table \ref{tab:compkvlstar} shows average performance statistics over 20 learning attempts for both learning algorithms. The models were learned with retransmission-filtering enabled. The same hardware and software configurations were used as described in Section \ref{sec:env} with the learning program set up on a VirtualBox 6.1 \ac{vm} allotted 4GB of memory and one CPU core. We used all the basic packets for our input alphabet, so
\emph{sa\_main}, \emph{key\_ex\_main}, \emph{authenticate}, \emph{sa\_quick} and \emph{ack\_quick}. The model learned is the clean model seen in Figure \ref{fig:reference}. Table \ref{tab:compkvlstar} shows the metric on the left and the respective averages for the $L^*$ and $KV$ learning algorithms respectively on the right. Interesting results are highlighted in bold. From top to bottom, the metrics measured are as follows. Learning rounds refers to the number of rounds the learning algorithms had to run for, or in other words, how many attempts they needed to correctly learn the \ac{sul}. Total time is the total time needed by the algorithm from start to the finished model. The total time can be split into time spent on the learning algorithm and time spent on equivalence queries. Learning membership queries refers to the number of membership queries sent to the SUL while learning steps to the steps in the learning algorithm itself. Analogously, equivalence oracle queries refers to the equivalence queries sent to the SUL and equivalence oracle steps to the steps needed by the equivalence oracle implementation. Finally, membership queries saved by caching details the performance boost gained by caching membership queries, with the value indicating the number of queries saved.

As the only difference between the two configurations tested was the choice of learning algorithm, intuitively one expects relevant fields to vary the most with equivalence oracle field to be largely unchanged. This intuition is confirmed by our experiments, wherein while the time spent on equivalence queries was very similar, with both requiring the same number of equivalence oracle queries for conformance checking. In contrast, the time spent on membership queries differs greatly between the two model learning algorithms. The $L^*$ algorithm required almost double the number of membership queries than its $KV$ counterpart. As communication with the \ac{sut} is the main performance bottleneck and membership queries make up a large portion of this communication, this change naturally led to a significantly better runtime for $KV$, with total time spent on the learning algorithm being close to half that of the $L^*$ algorithm. This difference in time spent on the learning algorithm meant, that for this experiment, the $KV$ algorithm learned a model in roughly 75\% of the time needed by the $L^*$ algorithm. Looking only at the learning algorithm, $KV$ performed roughly twice as well as its counterpart. As the same equivalence checking algorithm was used for both attempts, the identical number of equivalence oracle queries makes sense. Another noticeable difference can be observed in the number of membership queries saved by caching. Here, $KV$ saves more than double the amount $L^*$ does, indicating a better caching implementation. In summation, we found the $KV$ algorithm to be better suited for our learning setup and solely used it for fuzzing. 

Little variance \todo{show standard deviation} was observed throughout all learning attempts so the sample size of 20 learning attempts each is believed to be representative. However, for even more accurate results the experiment should be carried out again for even more runs. Additionally, it might be interesting to compare the performance of various equivalence oracles for this learning setup.

\begin{table}[h]
	\centering
	\begin{tabular}{ |p{6.5cm}||p{1cm}|p{1cm}|  }
		\hline
		\multicolumn{3}{|c|}{\textbf{Learning Algorithm Performance (Averages)}} \\
		\hline
		\textbf{Metric} & $\mathbf{L^*}$ & $\mathbf{KV}$ \\
		\hline
		Learning Rounds							&	2				&	4 				\\
		Total Time (s)							&   1652			& 	1214   			\\
		Time Learning Algorithm	(s)				&	\textbf{899}	& 	\textbf{480}	\\
		Time Equivalence Checks (s)				& 	753				& 	734			\\
		Learning Membership Queries 			&   \textbf{177}	& 	\textbf{78}		\\
		Learning Steps							& 	856	  			& 	676   			\\
		Equivalence Oracle Queries				& 	60  			&  	60				\\
		Equivalence Oracle Steps				& 	747  			&  	934				\\
		Membership Queries Saved by Caching		& 	\textbf{13}		&  	\textbf{30}				\\
		\hline
	\end{tabular}
	\caption{Comparison $L^*$ and $KV$}
	\label{tab:compkvlstar}
\end{table}
\newpage

\subsection{Library Error} \label{subsec:liberror}
% section with discovered bug
Another notable finding from the model learning phase, which demonstrates the usefulness of \ac{aal} from a testing standpoint, was the discovery of a bug in a used Python Diffie-Hellman key exchange library. The bug was only found thanks to the exhaustive number of packets sent with our mapper class and due to the non-determinism checks implemented in \textsc{AALpy}. Despite our best efforts in removing the non-deterministic behavior from our learning process, we would still get occasional non-determinism errors at random points while learning. This problem persisted over several weeks due to the fact that the errors occurred randomly and only sporadically during some learning attempts. Initially we believed this to be also caused by retransmissions, but since the problems persisted even after introducing retransmission-filtering, that possibility was ruled out. The other option was of course problems in our implementation of the \ac{ipsec} protocol. Therefore, a lot of time was invested into painstakingly comparing logs and packet captures between our implementation and the \ac{sul} to ensure that everything lined up, since \textsc{AALpy} was still reporting non-determinism errors. Finally, a small discrepancy between the two logs was discovered and through it, that the problems were not in fact caused by our implementation, but by a used Python library. It turns out there was a very niche bug in a used Diffie-Hellman Python library~\cite{topdappdh} where, if the most significant byte was a zero, it would be omitted from the response, causing the local result to be one byte shorter than the value calculated by the \ac{sul}. As this would only occur in the rare case where the MSB of the DH exchange was zero, this explains the random and difficult to reproduce nature of the bug. This behavior was undocumented and happened in a function call that allowed specifying the length of the returned key. As the library is not a very widespread one, the impact of this bug is presumably not very high. Regardless, it could compromise the security of affected systems and therefore the maintainer of the library has been notified of the problem. Due to the elusive nature of this bug, it would very likely not have been noticed without the exhaustive communication done by the model learning process and without seeing the slight differences in the resulting models that did not crash during the learning process.

\section{Fuzzing Results} \label{sec:fuzzresults}
We used model-based fuzzing to test the \ac{ipsec} \ac{ike}v1 \ac{sut}. Our fuzzer supports testing inputs in the context of runs, to ensure an identical state on the \ac{sut} for each fuzzed input. To that end, we developed a custom fuzzer supporting two methods of generating the input sequences to be tested, filtering-based and mutation-based input sequence generation, as described in Chapter~\ref{chap:Fuzzing}. All fuzzing took place in the same isolated network described in Section~\ref{sec:env}. The used reference model can be seen above in Figure~\ref{fig:reference}. This section presents an evaluation of the results of using our custom fuzzer to fuzz a Strongswan \ac{ipsec} server. Two different input sequence generation methods were used and contrasted, comparing performance and findings. Both methods found the same issues, however the mutation-based input sequence generation method proved to be significantly faster.

\subsection{Findings} \label{subsec:findings}
Our fuzzer was very successful in finding new states, discovering new cases for almost every input sequence tested with both run-generation methods. As a new state found does not necessary indicate, that the new behavior is harmful, the discovered new states still had to be looked over for particularly interesting behavior. By analyzing the run-fuzzed input combinations leading to new states, we discovered two undocumented instances of the \ac{sut} not following RFC specifications. Unfortunately, a lot of ``non-findings'' were discovered as well, non-findings referring to new states not included in the reference model, but also not exhibiting undocumented or otherwise interesting behavior. Some of these non-findings could be removed by simply specifying some fields which should not be fuzzed. Unfortunately, this approach requires first manually going through and verifying that none of the discovered behavior is interesting. As an alternative approach, the fuzzer could be further improved in future work by reducing the amount of noise by either relearning the reference model with a more advanced mapper class, or by adding newly learned states as soon as they are discovered to avoid the many duplicate new states found. \todo[inline]{Want to still try this}
For completeness sake, we first present some of the more interesting or common non-findings, followed by the two discovered deviations from the RFC specifications. 

An example of a non-finding that is typically categorized as noise is the discovery of new states by changing the initiator/responder cookies during the run. These cookies are used in part to identify the members of a \ac{vpn} connection. The new states were found, as during the learning of the reference model, the cookies were not changed mid-run, as this causes the \ac{sut} to think it is communicating with a different user and if it doesn't know that user, to simply discard the message. As fuzzing the cookie fields did not lead to any errors on the server side and greatly complicated the mapper class, this field was removed from the fuzzing scope. 

Another non-finding was discovered while fuzzing the field indicating the number of proposals in the \emph{ISAKMP SA} packet. While testing various randomly chosen lengths, every time the length was set to one, the fuzzer indicated, that a new state had been found. This behavior was caused due to our implementation of both the fuzzer and base reference model. Our \emph{ISAKMP SA} packet always contains exactly one proposal and the packet being fuzzed is assumed to be incorrect. However, in this case, the field is, by fuzzing through random numbers, every so often set to a valid value (namely one), resulting in a mismatch between the expected (error) and actual (normal) response. Yet another cause of many non-findings during fuzzing were the various packets containing hashes, e.g. \emph{ACK} and \emph{AUTH}. Fuzzing these hashes proved to be problematic, as random changes to the hashes cause the server responses to not correspond correctly to the mapper class, making decryption impossible. Therefore, errors had to be manually examined in the Strongswan logs. Seeing as no crashes or other unexpected errors were observed, fuzzing hashes was reduced in the overall fuzzer. 

Overall, while looking through and understanding the many new states helped improve our overall understanding of the \ac{ipsec} protocol, most of them failed to exhibit any undefined/unexpected behavior. However, in two separate cases, deviations from the relevant RFC specifications were observed with the \ac{sut}. The first of these was discovered while fuzzing the \ac{isakmp} length field. When manually going through the results of the fuzzer, a significant amount of newly discovered states were found to have been caused by packets with a fuzzed \ac{isakmp} length field. The response to the fuzzed packet was compared to the expected return value according to the reference model. The two values did not conform, causing the fuzzer to indicate that a new state had been found. The mismatch was caused by the reference model expecting an error response to the incorrect \ac{isakmp} length field, while in practice, the \ac{sut} ignored the content of the field entirely. This behavior is showcased in Listing~\ref{lst:finding_isalen}, Line 6, where an error was expected, but the \ac{sut} returned a valid response, despite the length field being the obviously incorrect value of hex $FF000000_{16}$ ($4278190080_{10}$). The finding was similarly observed with all other tested values, in all ranges from zero to $FFFFFFFF_{16}$, leading us to the conclusion, that the field is in fact completely ignored.

The impact of this finding is presumably rather small, however it might lead to inconsistencies between different \ac{ipsec} implementations, should they handle this behavior differently. Additionally, RFC 2048 (describing \ac{isakmp}) clearly specifies, that the \ac{isakmp} payload length field must match the length of the entire payload. Seeing as \ac{ipsec} simply builds on \ac{isakmp}, these requirements should still hold true. In fact, RFC 2409, Section 5~\cite{rfc:ikev1} even states very plainly, that \ac{ike}v1 exchanges are to conform to the \ac{isakmp} standard.
This standard, in Section 5.1 of RFC 2048~\cite{rfc:isakmp} states the following. 

\begin{quotation}
	...If the ISAKMP message length and the value in
	the Payload Length field of the ISAKMP Header are not the same, then
	the ISAKMP message MUST be rejected.
\end{quotation}

In other words, according to the RFC, the length field-manipulated \ac{isakmp} packets are invalid and should be rejected. Since the tested Strongswan server does not, this finding falls into the deviations from specifications category.


\begin{lstlisting}[float=h, caption=Finding showing the ISAKMP length field being ignored, label=lst:finding_isalen]
	Fuzzing isa_len with: b'\xff\x00\x00\x00'
	Run: ['sa_main_fuzz', 'key_ex_main', 'authenticate', ...]
	$sa_main_fuzz
	%%%%%%%%%%%%%%%%%%%%
	**********
	Expected: ERROR_NOTIFICATION | Received: ISAKMP_SA
	**********
	
	$key_ex_main
	**********
	Expected: None | Received: ISAKMP_KEY_EX
	**********
	
	$authenticate
	**********
	Expected: None | Received: ISAKMP_AUTH
	**********
	...
\end{lstlisting}


Another interesting finding can be observed when fuzzing the transform \texttt{Authentication} field sent at the start of an \ac{ike} exchange as part of an \emph{ISAKMP SA} packet. The expected behavior for the fuzzed field was, that the server would return an error response right away, indicating it does not support the proposed unknown \texttt{Authentication} method. However, in practice, the error response was only received after the following key exchange packet was sent. Strongswan logs also do not show any errors when parsing in an \emph{ISAKMP SA} packet with a fuzzed \texttt{Authentication} transform field. This is due to Strongswan apparently only verifying the content of the \texttt{Authentication} transform field during the key-exchange step. RFC 2049, section 5~\cite{rfc:ikev1} explicitly states that 

\begin{quotation}
	Exchanges conform to standard ISAKMP payload syntax, attribute
	encoding, timeouts and retransmits of messages, and informational
	messages-- e.g a notify response is sent when, for example, a
	proposal is unacceptable, or a signature verification or decryption
	was unsuccessful, etc.
\end{quotation}

In particular the second part abort a notification being sent for unacceptable proposals leads us to believe, that the behavior exhibited by the Strongswan \texttt{Authentication} transform field is unintended behavior, that deviates from the RFC specification. It is important to note, that only the \texttt{Authentication} field exhibited this behavior, all the other tested transform fields (\texttt{Encryption}, \texttt{KeyLength}, \texttt{Hash}, \texttt{Group Description}, \texttt{Life Type} and \texttt{Life Duration}) appear to be checked right away and return errors if invalid/unknown. Another interesting point to note, is that Wireshark logs of the sent packets show, that the response transform has the \texttt{Authentication} field set to \ac{psk}, despite the fuzzed value sent to it not being supported. This indicates, that for this field Strongswan reverts to a default value if it encounters an invalid input, without logging any errors.

While not necessarily severe findings, the two RFC specification deviations clearly do not conform to \ac{isakmp} and \ac{ipsec} standards. As deviations from specifications can lead to vulnerabilities and compatibility issues, they should be carefully reviewed to ensure, that they are on purpose and not due to an oversight. In particular the \texttt{Authentication} field finding should be checked, as all other fields of that packet exhibited the correct behavior, indicating a potential developer oversight. It is important to thoroughly examine any deviations from established standards to ensure that the system is as secure as possible.


\subsection{Mutation vs. Filtering-based Input Sequence Generation} \label{subsec:mutation_vs_filtering}
This subsection compares the performance of the two input sequence generation methods used for fuzzing. Both methods discovered the same new states and therefore also findings. The main differences between the two methods lies in the amount of coverage achieved and runtime. While the filtering-based method achieves greater coverage over more states due to testing a large amount of different runs, the runtime is also proportionally longer. Table~\ref{tab:compfuzz} shows a comparison of the two used input sequence generation methods, with the mutation-based method being run for a total of 60 mutations. The ``Runs'' column refers to the number of input sequences generated, while the ``Run Length'' column indicates the average amount of inputs per run. The ´´Seconds / Run'' column displays the average execution time of a single input sequence and the ´´Values Tested'' column shows the total number of fuzzed values tested in runs. Finally, the ´´Runtime'' column indicates the total runtime of the methods. These metrics allow for a comparison of the efficiency and effectiveness of the two methods in generating and testing input data.

\begin{table}[]
	\centering
	\begin{tabular}{|l|l|l|}
		\hline
		\rowcolor[HTML]{EFEFEF} 
		& \textbf{Mutation} & \textbf{Filtering} \\ \hline
		\textbf{Runs}              & 1                 & 175                \\ \hline
		\textbf{Run Length (Av. )} & 11                & 16                 \\ \hline
		\textbf{Seconds / Run}     & 11                & 16                 \\ \hline
		\textbf{Values Tested}     & 2522              & 44000              \\ \hline
		\textbf{Runtime (h)}       & $\sim$8           & $\sim$194          \\ \hline
	\end{tabular}
	\caption{Comparison Mutation-based and Filtering-based Fuzzing}
	\label{tab:compfuzz}
\end{table}

Examining Table~\ref{tab:compfuzz}, the most striking difference between the two methods is the difference in number of input sequences generated and the total runtime. While the mutation-based method always only generates a single run, the filtering-based approach generates far more, in our case 175 runs. For the input sequence generation with 60 mutations, the resulting input sequence consisted of 11 inputs. In contrast, the average input sequence length for the filtered input sequences was 16 inputs. The ``Run Length'' translates directly to the  ´´Seconds / Run'' metric. Each further input leads to, on average, an additional second of runtime, due to timed waits and timeouts during network communication. As every field of each input is fuzzed when utilizing mutation-based input sequence generation, the amount of values tested per input sequence is significantly higher for this method. In comparison, only roughly 250 fuzzed values are tested per input sequence when using the filtering-based input sequence generation method. Still, as 175 input sequences have to be tested, despite the significantly less fuzzed values per run, the total amount of values tested is still far larger with the filtering-based method. In fact, a comparison of the two entries in the ´´Runtime'' column shows that the filtering-based method takes approximately 24 times longer to complete than the mutation-based method. The runtime increases proportionally with the amount of tested inputs and average input sequence length.